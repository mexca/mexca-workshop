{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mexca/mexca-workshop/blob/main/notebooks/20240214_mexca_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbkj_EAxFGQQ"
      },
      "source": [
        "# Example: Emotion Feature Extraction With Mexca\n",
        "\n",
        "In this notebook, we will use the US presidential debate between Clinton and Trump in 2016 as an example to capture and compare emotion expressions with MEXCA. The video can be found on [YouTube](https://www.youtube.com/watch?v=DBhrSdjePkk), but we will use a file that is hosted by a third party.\n",
        "\n",
        "The video contains three persons (Hillary Clinton, Donald Trump, and a moderator), but we will use a part of the video in this example where only Clinton and Trump are present. Most frames of the video contain a least one face and speech. Because it contains only a limited number of faces and speakers and the faces are mostly shown in close-ups, the video is a good example to demonstrate MEXCA."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation\n",
        "\n",
        "It is recommended to run MEXCA on a GPU, so we need to make sure that a GPU is available. In Google Colab, we can do this by chaning the runtime type (under `Runtime` select `Change runtime type` and `T4 GPU`)."
      ],
      "metadata": {
        "id": "MPnyRAFFgmtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we begin, we must install the full version of the mexca Python package. This can be done with `pip install mexca[all]`. The `[all]` appendix indicates that all components of the MEXCA pipeline should be installed. The installation can take a few minutes to finish. (Note that `!` is an IPython magic command to run a line as a shell command)."
      ],
      "metadata": {
        "id": "W6VFDpJRYGES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mexca[all]\n",
        "\n",
        "# Fixes a bug with Colab and triton package\n",
        "!pip install --no-deps \"triton==2.0.0\""
      ],
      "metadata": {
        "id": "CO9oURzDYKPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check if the installation was successful, we can try to access the version of the installed mexca package."
      ],
      "metadata": {
        "id": "td6WT9AoV_0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mexca\n",
        "\n",
        "mexca.__version__ # Should return `1.0.1`"
      ],
      "metadata": {
        "id": "qzVEMYuSWZOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check if a GPU is available using `torch.cuda.is_available()`."
      ],
      "metadata": {
        "id": "QwS_kIzt0Ynk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available() # Should return `True` if GPU is available"
      ],
      "metadata": {
        "id": "4HLhlfFyTqwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now import the required packages for the remainder of the notebook."
      ],
      "metadata": {
        "id": "dxO0-C4IZKvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vF37KDLFGQS"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import yaml\n",
        "\n",
        "from base64 import b64encode\n",
        "from google.colab import userdata\n",
        "from urllib.request import urlopen\n",
        "\n",
        "from mexca.audio import SpeakerIdentifier, VoiceExtractor\n",
        "from mexca.pipeline import Pipeline\n",
        "from mexca.text import AudioTranscriber, SentimentExtractor\n",
        "from mexca.video import FaceExtractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L99A2l0OFGQT"
      },
      "source": [
        "We also need to download the example video file from the third party URL. First, we define a function to download a file from an URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTGDNjuEFGQT"
      },
      "outputs": [],
      "source": [
        "def download_example(url, filename):\n",
        "    # Check if filename exists\n",
        "    if not os.path.exists(filename):\n",
        "        video = urlopen(url)\n",
        "\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(video.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we specify the URL, a name for the video file, and use the download function."
      ],
      "metadata": {
        "id": "ToAgA878aXCx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGMynhY8FGQT"
      },
      "outputs": [],
      "source": [
        "video_url = 'https://books.psychstat.org/rdata/data/debate.mp4'\n",
        "filename = 'debate.mp4'\n",
        "\n",
        "download_example(video_url, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We can run `os.path.exists()` to check if the video was successfully downloaded."
      ],
      "metadata": {
        "id": "M1Ua0j63d6BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.exists(filename) # Should return `True`"
      ],
      "metadata": {
        "id": "YXSldZ_4d_4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpZxsvXxFGQU"
      },
      "source": [
        "## Building the Pipeline\n",
        "\n",
        "Next, we build the pipeline by combining different components for video, audio, and text processing. We first define the number of persons shown in the part of the video that we will analyze (Clinton and Trump). Setting the number of faces and speakers correctly is important as emotion expression features might be attributed to the wrong person otherwise."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = 2"
      ],
      "metadata": {
        "id": "f8RpHzn1xIG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we set the device on which the pipeline components should be run. If a GPU is available, we will use it. Otherwise, we will run the components on the CPU."
      ],
      "metadata": {
        "id": "rXeFq0k_xIuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    torch.device(type=\"cuda\")\n",
        "    if torch.cuda.is_available()\n",
        "    else torch.device(type=\"cpu\")\n",
        ")"
      ],
      "metadata": {
        "id": "DIYFMdBtxS8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To detect and extract features from faces shown in the video, we create the `FaceExtractor` component. We set `num_faces=num_clusters` so that detected faces will be assigned to two clusters based on their encoded representations (embeddings). We also add that the component should run on our specified device."
      ],
      "metadata": {
        "id": "hqVMtpP8ww9k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHTS6VIzFGQU"
      },
      "outputs": [],
      "source": [
        "face_extractor = FaceExtractor(\n",
        "    num_faces=num_clusters,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JM4KCacFGQU"
      },
      "source": [
        "For the audio processing, we create two components: The `SpeakerIdentifier` detects speech segmenets in the audio signal and assigns them to speaker clusters. As with the faces, we assume that the video has two speakers, so we set `num_speakers=num_clusters`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmWSPYBHFGQV"
      },
      "source": [
        "*Note*: mexca builds on pretrained models from the pyannote.audio package. Since release 2.1.1, downloading the pretrained models requires the user to accept two user agreements on Hugging Face hub and generate an authentication token. Therefore, to run the mexca pipeline, please accept the user agreements on [here](https://huggingface.co/pyannote-speaker-diarization-3.1) and [here](https://huggingface.co/pyannote/segmentation-3.0). Then, generate an authentication token [here](https://huggingface.co/settings/tokens). Use this token as the value for `use_auth_token` (instead of `HF_TOKEN`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "except:\n",
        "    raise Exception(\"Please generate your own access token for Hugging Face Hub\")"
      ],
      "metadata": {
        "id": "iHw5ERBTO8Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bNHBDb_FGQV"
      },
      "outputs": [],
      "source": [
        "speaker_identifier = SpeakerIdentifier(\n",
        "    num_speakers=num_clusters,\n",
        "    device=device,\n",
        "    use_auth_token=HF_TOKEN\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cSjN7YjFGQV"
      },
      "source": [
        "The `VoiceExtractor` computes vocal emotion expression features from the audio stream of the video. The configuration of the extracted voice feature set can be changed by setting `config=mexca.data.VoiceFeaturesConfig()`, but we will keep the default configuration for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoTSFQe9FGQV"
      },
      "outputs": [],
      "source": [
        "voice_extractor = VoiceExtractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJvMMtPrFGQV"
      },
      "source": [
        "To extract the sentiment from the spoken text, we create two text processing components. First, we transcribe the audio signal to text using the `AudioTranscriber` class. The component automatically detects the spoken language of each speech segment. The transcribed text is split into single sentences. The transcription is done using a Whisper model which comes in different sizes. Larger sized models make in most cases more accurate transcriptions but take longer to run. We set the size of the model with `whisper_model=\"medium\"` to use a medium sized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc17UQf1FGQW"
      },
      "outputs": [],
      "source": [
        "audio_transcriber = AudioTranscriber(\n",
        "    whisper_model=\"medium\",\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQPQTB2RFGQW"
      },
      "source": [
        "Second, the `SentimentExtractor` predicts a positive, negative, and neutral sentiment score for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy16BzeMFGQW"
      },
      "outputs": [],
      "source": [
        "sentiment_extractor = SentimentExtractor(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jwOTBEOFGQW"
      },
      "source": [
        "Now, we combine the five components into a `Pipeline` instance, which will run them after each other and integrate the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmQmgvETFGQW"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(\n",
        "    face_extractor=face_extractor,\n",
        "    speaker_identifier=speaker_identifier,\n",
        "    voice_extractor=voice_extractor,\n",
        "    audio_transcriber=audio_transcriber,\n",
        "    sentiment_extractor=sentiment_extractor\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAGYN-znFGQW"
      },
      "source": [
        "To track the progress of the pipeline, we create a logger to print messages at the `INFO` level."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=\"INFO\")"
      ],
      "metadata": {
        "id": "kHKIFSEUAAFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGADjklcFGQW"
      },
      "source": [
        "To run the pipeline, we call the `apply()` method. The video has a frame rate of 25 and to speed up the processing, we choose to process 10 video frames at a time (`frame_batch_size=10`) and to only process every 5th frame (`skip_frames=5`), assuming that emotion expressions do not change substantially faster than 200ms. For this example, we also indicate to only process the first 30 seconds using `process_subclip=(0, 30)`.\n",
        "\n",
        "**Note**: The first time you run the pipeline pre-trained models will be automatically downloaded which can take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqJXDGtwFGQW"
      },
      "outputs": [],
      "source": [
        "output = pipeline.apply(\n",
        "    filename,\n",
        "    frame_batch_size=10,\n",
        "    skip_frames=5,\n",
        "    process_subclip=(0, 30)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8pqWdzzFGQW"
      },
      "source": [
        "To simplify further processing, we store the `features` attribute from the output of the pipeline which contains the integrated features as a `polars.lazyframe.frame.LazyFrame` in a separate variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYfxpFMOFGQW"
      },
      "outputs": [],
      "source": [
        "output_df = output.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si_ufvIJFGQX"
      },
      "source": [
        "We can get a quick glimpse of the output by calling polars' `describe` method. Note that some columns contain lists as row elements (e.g., `face_box`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7H3VnzaFGQX"
      },
      "outputs": [],
      "source": [
        "output_df.collect().describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVqx6lgAFGQX"
      },
      "source": [
        "## Analyzing Facial Expressions\n",
        "\n",
        "We start analyzing the output by comparing facial action unit activations between Clinton and Trump."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIpXavfWFGQX"
      },
      "outputs": [],
      "source": [
        "def stderr(x):\n",
        "    \"\"\"Calculate the standard error of the mean\n",
        "    \"\"\"\n",
        "    return np.std(x)/np.sqrt(len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aAu3j4YFGQX"
      },
      "outputs": [],
      "source": [
        "clinton_id = 1\n",
        "trump_id = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc4BDkeqFGQX"
      },
      "outputs": [],
      "source": [
        "n_au = 27 # Nr of action units\n",
        "\n",
        "# Expand action unit lists into separate rows\n",
        "au_df = output_df.select(\n",
        "    [\n",
        "        pl.lit([list(range(27))]).alias(\"face_au\"),\n",
        "        pl.col(\"face_aus\").alias(\"value\").list.take(list(range(27))),\n",
        "        pl.col(\"face_label\")\n",
        "    ]\n",
        ").filter(pl.col(\"value\").is_not_null()).explode([\"face_au\", \"value\"])\n",
        "\n",
        "# Compute mean and standard error for each action unit\n",
        "au_stats = (\n",
        "    au_df\n",
        "    .groupby('face_label', \"face_au\")\n",
        "    .agg(\n",
        "        pl.mean(\"value\").alias(\"avg\"),\n",
        "        (pl.std(\"value\")/pl.count().sqrt()).alias(\"ste\")\n",
        "    )\n",
        "    .sort(\"face_au\", \"face_label\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldwqyOZaFGQX"
      },
      "outputs": [],
      "source": [
        "# Reference ids of the action units\n",
        "au_ref = [1, 2, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22,\n",
        "          23, 24, 25, 26, 27, 32, 38, 39]\n",
        "\n",
        "aus = np.arange(n_au)\n",
        "\n",
        "# Create bar plot with error bars\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "clinton_au_df = au_stats.filter(\n",
        "    pl.col(\"face_label\") == str(clinton_id)\n",
        ").collect()\n",
        "trump_au_df = au_stats.filter(\n",
        "    pl.col(\"face_label\") == str(trump_id)\n",
        ").collect()\n",
        "\n",
        "ax.bar(aus-width/2, clinton_au_df.select(pl.col(\"avg\")).to_series(), width,\n",
        "       yerr=1.96*clinton_au_df.select(pl.col(\"ste\")).to_series(),\n",
        "       capsize=4, ecolor='darkgray', label='Clinton', color='seagreen')\n",
        "\n",
        "ax.bar(aus+width/2, trump_au_df.select(pl.col(\"avg\")).to_series(), width,\n",
        "       yerr=1.96*trump_au_df.select(pl.col(\"ste\")).to_series(),\n",
        "       capsize=4, ecolor='darkgray', label='Trump', color='indianred')\n",
        "\n",
        "ax.set_xlabel('Action unit')\n",
        "ax.set_xticks(aus, au_ref)\n",
        "ax.set_ylabel('Mean activation (95% CI)')\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL020WOgFGQX"
      },
      "source": [
        "The FaceExtractor compoenent extracts the activations of 41 action units. Here we only select the first 27 which are bilateral units (the following 14 units correspond to left and right unilateral activations). The bar plot shows substantial differences between Clinton and Trump in the mean activations of units associated with joy (6 and 12; Trump higher than Clinton). There are also differences in units related to sadness (1 and 4; Clinton higher than Trump). Moreover, Clinton shows higher mean activations related to fear (1, 4, 5, and 20) and for anger-related units (4, 5, 23). Note that these results must interpreted with care, as we are not comparing the activations against a reference data base or baseline.\n",
        "\n",
        "## Analyzing the Voice\n",
        "\n",
        "Besides facial emotion expressions, mexca also allows us to analyze vocal expressions. By default, it extracts the voice pitch measured as the fundamental frequency F0 from speakers in the video which indicates emphasis and is related to emotional arousal. Similar to the action units, we can compare voice pitch between Clinton and Trump. For an overview of all voice features, see the [documentation](https://mexca.readthedocs.io/en/latest/output.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get speaker IDs, pitch and time colulmns\n",
        "segment_speaker, pitch, time = output_df.select(\n",
        "    pl.col(\"segment_speaker_label\", \"pitch_f0_hz\", \"time\")\n",
        ").collect().to_numpy().T\n",
        "\n",
        "# Set non-speaker frames to NaN to avoid lines connecting separate speech segments\n",
        "clinton_time = time.copy()\n",
        "clinton_time[segment_speaker != str(clinton_id)] = np.nan\n",
        "clinton_pitch = pitch.copy()\n",
        "clinton_pitch[segment_speaker != str(clinton_id)] = np.nan\n",
        "\n",
        "trump_time = time.copy()\n",
        "trump_time[segment_speaker != str(trump_id)] = np.nan\n",
        "trump_pitch = pitch.copy()\n",
        "trump_pitch[segment_speaker != str(trump_id)] = np.nan"
      ],
      "metadata": {
        "id": "eJKBo9p6D5mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snQFzk3JFGQX"
      },
      "outputs": [],
      "source": [
        "# Create line plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(clinton_time, clinton_pitch, label='Clinton', color='seagreen')\n",
        "ax.plot(clinton_time, [0] * clinton_time.shape[0], color = 'seagreen')\n",
        "ax.axhline(np.nanmean(clinton_pitch), ls='--', color='seagreen')\n",
        "\n",
        "ax.plot(trump_time, trump_pitch, label='Trump', color='indianred')\n",
        "ax.plot(trump_time, [-5] * trump_time.shape[0], color = 'indianred')\n",
        "ax.axhline(np.nanmean(trump_pitch), ls='--', color='indianred')\n",
        "\n",
        "ax.set_xlabel('Time (in s)')\n",
        "ax.set_xticks(np.arange(35, step=5.0))\n",
        "ax.set_ylabel('Pitch (F0 in Hz)')\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U01AvGAkFGQY"
      },
      "source": [
        "The figure shows the voice pitch of Clinton and Trump over time and displays the mean pitch (dashed line). It shows that the baseline pitch of Trump's voice is higher on average than Clinton's.\n",
        "\n",
        "## Analyzing the Text\n",
        "\n",
        "Next to facial expressions and voice features, mexca can also extract the sentiment from the spoken text. Again, we can compare the positive, negative, and neutral sentiment in the speech content between Clinton and Trump."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text sentiment\n",
        "sent_pos, sent_neg, sent_neu = output_df.select(\n",
        "    pl.col(\"span_sent_pos\", \"span_sent_neg\", \"span_sent_neu\")\n",
        ").collect().to_numpy().T"
      ],
      "metadata": {
        "id": "LguW4XiMJI3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JObzFlHWFGQY"
      },
      "outputs": [],
      "source": [
        "# Create line plot\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1)\n",
        "\n",
        "ax1.plot(clinton_time, sent_pos, label='Clinton', color='seagreen')\n",
        "ax1.plot(trump_time, sent_pos, label='Trump', color='indianred')\n",
        "ax2.plot(clinton_time, sent_neg, label='Clinton', color='seagreen')\n",
        "ax2.plot(trump_time, sent_neg, label='Trump', color='indianred')\n",
        "ax3.plot(clinton_time, sent_neu, label='Clinton', color='seagreen')\n",
        "ax3.plot(trump_time, sent_neu, label='Trump', color='indianred')\n",
        "\n",
        "ax1.set_title('Positive')\n",
        "ax2.set_title('Negative')\n",
        "ax3.set_title('Neutral')\n",
        "ax3.set_xlabel('Time (in s)')\n",
        "for ax in (ax1, ax2, ax3):\n",
        "    ax.set_xticks(np.arange(35, step=5.0))\n",
        "    ax.set_yticks(np.arange(1.2, step=0.2))\n",
        "ax2.set_ylabel('Sentiment score')\n",
        "ax2.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX90jvqNFGQY"
      },
      "source": [
        "We can see that Clinton uses relatively neutral sentiment. Trump, in contrast, has a strongly positive peak in his turn when he talks about \"the finest deal you've ever seen\" and a negative peak at \"all of a sudden you were against it\".\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print transcription at peak time window\n",
        "with pl.Config(fmt_str_lengths=100):\n",
        "    print((output_df\n",
        "        .filter(\n",
        "            pl.col(\"time\").is_between(22, 27) &\n",
        "            pl.col(\"span_text\").is_not_null()\n",
        "        )\n",
        "        .select(pl.col(\"time\", \"span_text\"))\n",
        "        .unique(subset=\"span_text\")\n",
        "        .sort(\"time\")\n",
        "    ).collect())"
      ],
      "metadata": {
        "id": "bHw5kJetJ5d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydFpb80KFGQY"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this example, we build a custom pipeline using the mexca package to extract emotion expressions from a video. We ran the pipeline on an excerpt from the US presidential debate 2016 between Clinton and Trump. We analyzed differences in facial action unit activations, voice pitch, and speech text sentiment between the two candidates.\n",
        "\n",
        "## References\n",
        "\n",
        "Lüken, M., Moodley, K., Viviani, E., Pipal, C., & Schumacher, G. (2024, January 18). MEXCA - A simple and robust pipeline for capturing emotion expressions in faces, vocalization, and speech. https://doi.org/10.31234/osf.io/56svb"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "vscode": {
      "interpreter": {
        "hash": "cf45cf28e02693e1f07e3287e6807361380631608e21e3f53ef1041bfccc5ce4"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}